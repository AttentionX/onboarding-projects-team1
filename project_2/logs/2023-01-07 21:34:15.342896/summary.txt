
1. The authors tried to accomplish creating a new simple network architecture, the Transformer, that is solely based on attention mechanisms without any recurrence or convolutions. The Transformer was applied to two machine translation tasks, English-to-German and English-to-French, and was also applied to English constituency parsing with both large and limited training datasets.  
2. The key elements of the approach involve attention mechanisms which allow the model to focus on specific examples or elements of the input data. The mathematics behind the attention mechanisms involve computing the dot product of the query vector (created by the query matrix) and the key vectors (created by the key matrix) of the input data. The results of this computations are then used to create a weight vector which is used to obtain the output from the value matrix. 
3. The approach was limited by the size of the datasets since the training datasets were relatively small. Additionally, the attention mechanisms used in the Transformer did not consider any granularity of the input data which can lead to inaccuracies in the output. 
4. The Transformer could be used for Computer-Assisted Language Learning in order to assist with machine translation tasks and automatic text categorization. The attention mechanisms would allow the model to focus on specific elements or examples in the input datasets and then generate output based on this focus. Additionally, the Transformer could be used to generate translations from one language to another. 
5. Other references to follow include Neural Machine Translation by Jointly Learning to Align and Translate (2016) by Dzmitry Bahdanau et al., Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2018), and Neural Machine Translation of Rare Words with Subword Units (2016) by E. Chorowski et al.