Q : hello?
A :  I understand the information provided. I'm ready to answer questions related to the paper.
Q : do you know about RETRO model?
A :  I'm not familiar with the RETRO model. However, from the information provided, I understand that the Transformer model is a sequence transduction model based on self-attention, which is superior to recurrent or convolutional neural networks. It has been applied to machine translation tasks and English constituency parsing with both large and limited training data.
Q : So you cannot answer the questions about RETRO model?
A :  I'm sorry, I'm not familiar with the RETRO model. However, from the information provided, I understand that the Transformer model is a sequence transduction model based on self-attention, which is superior to recurrent or convolutional neural networks. It has been applied to machine translation tasks and English constituency parsing with both large and limited training data. Variations on the Transformer architecture have been tested, and it was observed that reducing the attention key size hurts model quality, while bigger models are better and dropout is very helpful in avoiding over-fitting.
Q : OK, good. Then explain about the position encoding
A :  Position encoding is a way to inject information about the relative or absolute position of tokens in the sequence into the model. This is done by adding "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. The positional encodings are sine and cosine functions of different frequencies, with wavelengths forming a geometric progression from 2π to 10000 · 2π. This allows every position in the decoder to attend over all positions in the input sequence, mimicking the typical encoder-decoder attention mechanisms in sequence-to-sequence models. In the encoder, each position can attend to all positions in the previous layer of the encoder.
Q : Does transformer use relative position embedding?
A :  The Transformer model does not use relative position embedding. Instead, it uses a combination of sinusoidal positional encoding and learned positional embeddings. This allows the model to attend over all positions in the input sequence, mimicking the typical encoder-decoder attention mechanisms in sequence-to-sequence models. The Transformer has been applied to English constituency parsing with both large and limited training data.
Q : So transformer's position embedding is absolute, right?
A :  The Transformer model uses a combination of sinusoidal positional encoding and learned positional embeddings. This allows the model to attend over all positions in the input sequence, mimicking the typical encoder-decoder attention mechanisms in sequence-to-sequence models. The Transformer does not use relative position embedding, but rather absolute position embedding. Variations on the Transformer architecture have been tested, and it was observed that reducing the attention key size hurts model quality, while bigger models are better and dropout is very helpful in avoiding over-fitting.
Q : please answer in 'yes' or 'no'. is transformer's position encoding absolute?
A :  Yes, the Transformer model uses absolute position encoding. It uses a combination of sinusoidal positional encoding and learned positional embeddings, which allows the model to attend over all positions in the input sequence, mimicking the typical encoder-decoder attention mechanisms in sequence-to-sequence models. Variations on the Transformer architecture have been tested, and it was observed that reducing the attention key size hurts model quality, while bigger models are better and dropout is very helpful in avoiding over-fitting.
Q : Ok, thanks.
A :  On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. On the WMT 2014 English-to-French translation task, the Transformer model achieved a single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. The Transformer also generalizes well to other tasks, such as English constituency parsing, with both large and limited training data.
