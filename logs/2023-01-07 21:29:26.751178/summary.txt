 
1. The authors of this article attempted to create a new network architecture, the Transformer, which is based solely on attention mechanisms, instead of using the more complex recurrent or convolutional neural networks that include an encoder and a decoder. This architecture was tested on two machine translation tasks, and was found to be superior in quality, more parallelizable, and require less time to train than the existing models.

2. The key elements of the approach are attention mechanisms, which are used to connect the encoder and decoder without the need for recurrence or convolutions. These attention mechanisms use a combination of Multi-Head Attention and Position-wise Feed-Forward Networks to allow the model to learn the dependencies between different parts of the input sequence. Multi-Head Attention is used to calculate a weighted sum of the input sequence, which is then fed into a Position-wise Feed-Forward Network to generate a new sequence.

3. The main limitation of the approach is that it requires a large amount of data to train the model effectively. The authors used 8 GPUs to train their model on two machine translation tasks, but this may not be feasible for all applications. Additionally, the model may not be able to accurately translate more complex sentences due to the lack of recurrence or convolutions.

4. The Transformer could be used for computer-assisted language learning in a variety of ways. For example, it could be used to generate translations of sentences in different languages, allowing students to practice their language skills in a more interactive way. Additionally, it could be used to generate summaries of long texts, providing students with a quick overview of the material.

5. Other references to follow would include the original paper for the Transformer architecture, as well as any other papers related to attention mechanisms in natural language processing. Additionally, papers related to machine translation and natural language processing more generally could also be useful.