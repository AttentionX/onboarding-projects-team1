{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "aga9S1XPmNlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_KEY = "
      ],
      "metadata": {
        "id": "tvJNKmCPxFo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN9Gk5jDhz98"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "mkdir data\n",
        "export data_url=https://gist.githubusercontent.com/yujong-lee/18c53f033e80df8b56321b9a4764b332/raw/3cb7be84e1d6d18354183931902781041aebb50e/transformer.txt\n",
        "curl ${data_url} > data/transformer.txt\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "metadata": {
        "id": "zcCXQWGNjjgX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "metadata": {
        "id": "APSakCYujpIh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X GET \"localhost:9200/?pretty\" # Check Elasticsearch"
      ],
      "metadata": {
        "id": "d8x5x4oVjzf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "\n",
        "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
        "\n",
        "document_store = ElasticsearchDocumentStore(\n",
        "    host=host,\n",
        "    username=\"\",\n",
        "    password=\"\",\n",
        "    recreate_index=True,\n",
        "    index=\"document\",\n",
        "    embedding_field=\"emb\",\n",
        "    embedding_dim=768,\n",
        "    excluded_meta_data=[\"emb\"],\n",
        "    similarity=\"dot_product\",\n",
        ")"
      ],
      "metadata": {
        "id": "VMDsbV6TjvbV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import clean_wiki_text, convert_files_to_docs, fetch_archive_from_http\n",
        "from haystack.nodes import PreProcessor\n",
        "\n",
        "doc = convert_files_to_docs(dir_path=\"data\", clean_func=clean_wiki_text, split_paragraphs=True)\n",
        "\n",
        "preprocessor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=False,\n",
        "    split_by=\"word\",\n",
        "    split_length=200,\n",
        "    split_overlap=50,\n",
        "    split_respect_sentence_boundary=True,\n",
        "    language=\"en\"\n",
        ")\n",
        "docs = preprocessor.process(doc)\n",
        "\n",
        "print(len(doc), len(docs)) \n",
        "document_store.write_documents(docs)"
      ],
      "metadata": {
        "id": "VhEbxLSJkhNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store,\n",
        "    embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
        "    model_format=\"sentence_transformers\",\n",
        ")"
      ],
      "metadata": {
        "id": "PMyxYL_2lymi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_store.update_embeddings(retriever)"
      ],
      "metadata": {
        "id": "Y3J34s0YonYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import OpenAIAnswerGenerator\n",
        "\n",
        "# QA pair from T5 paper\n",
        "examples = [\n",
        "  [\n",
        "    'What did authors try to accomplish? Describe with rich examples.',\n",
        "    'The authors of this paper sought to explore the potential of transfer learning in natural language processing (NLP) by introducing a unified framework that converts all text-based language problems into a text-to-text format. They compared pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks and achieved state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.'\n",
        "  ],\n",
        "  [\n",
        "    'What were the key elements of the approach? Describe the mathematics behind the key elements as well with rich examples.',\n",
        "    'The key elements of the approach included the use of pre-training objectives, architectures, unlabeled datasets, and transfer approaches. The mathematics behind these elements involved the use of supervised and unsupervised learning methods to create models that could learn from labeled and unlabeled data. Additionally, the authors employed transfer learning techniques, such as fine-tuning pre-trained models, to further improve the accuracy of the models.'\n",
        "  ],\n",
        "  [\n",
        "    'In what ways the approach was limited by? Describe with rich examples.',\n",
        "    'The approach was limited by the amount of data available for pre-training and fine-tuning, as well as the difficulty of finding datasets with sufficient diversity to accurately model language understanding tasks. Additionally, the authors noted that transfer learning is not always effective for tasks that are more complex or require more nuanced understanding, such as sentiment analysis, text generation, and natural language inference.'\n",
        "  ],\n",
        "  [\n",
        "    'How could you use it for computer-assisted language learning? Describe with rich examples.',\n",
        "    'The approach could be used for computer-assisted language learning by pre-training a model on a language corpus or dataset and then fine-tuning the model on a language-specific task. This would allow the model to learn the languageâ€™s grammar and syntax and provide it with a better understanding of the language. Additionally, the model could be used to identify errors in student-generated text, providing feedback to the students to help them improve their language skills.'\n",
        "  ],\n",
        "  [\n",
        "    'What other references should you follow?',\n",
        "    'Other references that should be followed include the original paper by Devlin et al. (2018) on BERT, the paper by Radford et al. (2019) on GPT-2, and the paper by Conneau et al. (2019) on XLM. Additionally, the papers by Howard and Ruder (2018) and Peters et al. (2019) on ELMO and ULMFiT, respectively, can provide further insight into the use of transfer learning for NLP.'\n",
        "  ]\n",
        "]\n",
        "\n",
        "generator = OpenAIAnswerGenerator(\n",
        "    api_key=OPENAI_KEY,\n",
        "    model='text-davinci-003',\n",
        "    max_tokens=300,\n",
        "    top_k=1,\n",
        "    temperature=0.7,\n",
        "    frequency_penalty=0.3,\n",
        "    examples=examples,\n",
        ")"
      ],
      "metadata": {
        "id": "fry_Imzpnc6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipelines import GenerativeQAPipeline\n",
        "\n",
        "qa = GenerativeQAPipeline(generator=generator, retriever=retriever)"
      ],
      "metadata": {
        "id": "UYCLjV1amxHQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import print_answers\n",
        "\n",
        "questions = [\n",
        "  'What did authors try to accomplish? Describe with rich examples.',\n",
        "  'What were the key elements of the approach? Describe the mathematics behind the key elements as well with rich examples.',\n",
        "  'In what ways the approach was limited by? Describe with rich examples.',\n",
        "  'How could you use it for computer-assisted language learning? Describe with rich examples.',\n",
        "  'What other references should you follow? ',\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "  res = qa.run(\n",
        "    query=question,\n",
        "    params={\n",
        "      \"Retriever\": {\"top_k\": 3},\n",
        "      \"Generator\": {\"top_k\": 1},\n",
        "    },\n",
        "  )\n",
        "  print(question)\n",
        "  print_answers(res, details=\"minimum\")\n"
      ],
      "metadata": {
        "id": "0lZYSzYyoPfO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}