1. The Transformer is a new simple network architecture based solely on attention mechanisms.
2. Experiments on two machine translation tasks show that the Transformer is superior in quality, more parallelizable and requires less time to train.
3. The Transformer achieved 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results by over 2 BLEU.
4. On the WMT 2014 English-to-French translation task, the Transformer established a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs.
5. The Transformer generalizes well to other tasks, such as English constituency parsing with both large and limited training data.
6. The Transformer requires less time to train compared to existing models.
7. The Transformer is superior in quality compared to existing models.
8. The Transformer is composed of a stack of 6 identical layers, each layer has two sub-layers: a multi-head self-attention mechanism and a simple, positionwise fully connected feed-forward network.
9. The Transformer is auto-regressive, consuming the previously generated symbols as additional input when generating the next.
10. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.
11. The code used to train and evaluate the Transformer is available at https://github.com/tensorflow/tensor2tensor.
